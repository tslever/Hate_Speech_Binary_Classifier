You might consider adding this writing to the proposal. The proposal and below writing seem to be of a different format than the latest document Lorinda sent. I may add to this writing later.
 
We propose to either use unlabeled data such as the text of Reddit posts or their derivatives, or use labeled data such as "Online Hate Speech Reddit (2019)". Accordingly, we propose to compare the performance of models on one or more data sets and/or attempt to improve the performance of models on one or more data sets.
 
The main advantages of studying unsupervised deep learning are to avoid labeling text and to further a relatively unstudied field. However, supervised deep learning is generally more accurate. Additionally, unsupervised learning may fail with an unbalanced data set or result in clustering into classes different than hate speech / non-hate speech. Consider the below examples.
 
We could employ an autoencoder, a type of deep neural network, in hate-speech detection. Reconstruction error could be used as an anomaly score. Hate speech would be anomalous. An autoencoder is a deep neural network that studies data without labels. However, according to Yamanaka et al., "There are two main approaches in anomaly detection: supervised and unsupervised. The supervised approach accurately detects the known anomalies included in training data, but it cannot detect the unknown anomalies. Meanwhile, the unsupervised approach can detect both known and unknown anomalies that are located away from normal data points. However, it does not detect known anomalies as accurately as the supervised approach." Dr. Michael M. writes, "For 1-2 dimensional data, you can plot the data and visually identify outliers/anomalies as points far away from the rest. For very high dimensional data, unsupervised anomaly detection is close to being a hopeless task due to the curse of dimensionality, which - in the sense of anomaly detection - means that every point eventually becomes an outlier."
 
Generally clustering is not deep learning, but could employ deep learning. Facebook engineer Divam Gupta in 2019 wrote "Deep-Learning Based Clustering Techniques". Gupta writes, "In the past 3-4 years, several papers have improved unsupervised clustering performance by leveraging deep learning. Several models achieve more than 96% accuracy on MNIST dataset without using a single labeled datapoint. However, we are still very far away from getting good accuracy for harder datasets such as CIFAR-10 and ImageNet." We grant that this was 5 years ago and relates to clustering images.
 
Dr. Cinzia Viroli compares deep learning and Deep Gaussian Mixture Models (DGMM's). While separate, GMM's may be adding to Deep Neural Networks (DNNs). "Now [deep-learning methods] have attracted wide-spread attention; mainly for ‘supervised’ classification, where they very often outperform alternative learning methods". Viroli quotes G. Marcus: "In a world with infinite data, and infinite computational resources, there might be little need for any other technique." Viroli also describes issues with interpretability, computational effort, and the multiplicity of good models. DGMM "Model selection is an open issue... DGMM offers good result for [number of classes] h = 2 and h = 3... Computationally intensive for h > 3..." Dr. Dong Yu writes, "I will conclude the talk by indicating that DNNs not only do better but also are simpler than GMMs". Yu did not mention DGMM's. Dr. Nickolay Shmyrev writes, "GMM (Gaussian Mixture Model) and DNN (Deep Neural Networks) are two ways to classify every frame in the speech... GMM is faster to compute, easier to learn. GMM training is reliable, if you have a clean data, you are guaranteed to train a good system. DNNs are more accurate classifiers. At the same time DNN is very slow to train compared to GMM, they usually require GPU or lots of CPUs. They also slow to decode due to more computational complexity. Even with good data, DNN training is tricky because it's not guaranteed to converge to an optimal point. You usually have to apply several training tricks in order to make them converge to a good optimum." GMM's can be incorporated into DNN's.
 
https://divamgupta.com/unsupervised-learning/2019/03/08/an-overview-of-deep-learning-based-clustering-techniques.html#:~:text=In%20this%20post%2C%20I%20will,are%20assigned%20the%20same%20cluster.