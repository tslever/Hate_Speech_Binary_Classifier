I met with Dr. Kropko about the challenges of our course and a hate-speech binary classifier. Jon seemed to assume our hate-speech project is a supervised-learning problem. I grant that it seems like we can build supervised models on top of unsupervised BERT, for example. Jon also begged us not to label data as it would harm us psychologically and suggested that we evaluate the performance of a "simple" Random Forest.
 
Regarding data storage in general, Jon brought up MongoDB, traditional DBMS's, and DuckDB.
 
Jon suggested that while classifiers like Naive Bayes are coarse due to their "samples-are-independent" assumptions, such classifiers perform decently, and that we could flatten all of our chunks of text. Jon suggested that adding complexity is no guarantee of improved performance. Jon suggested we might include relative or absolute timestamps in chunks to provide temporal information, or the ID's of parent or child chunks. Regarding network information, Jon brought up graph databases and general-diffusion models. Jon suggested that including full parents or children would cause repetition, induced correlation, failure of independence assumptions, wrong estimates of uncertainty, less confidence, and less out-of-sample performance, but that Bayesian Hierarchical Models could account for repetitions of data.