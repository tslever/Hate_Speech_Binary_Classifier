{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2f0373a-b138-456a-a700-8313516b53a0",
   "metadata": {},
   "source": [
    "# Distilbert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc941c1-904a-4cec-87c1-ca325f526a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bd42d-1526-4709-9476-27cabd15f1e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b858c8-acfc-4114-ad52-16f4e70bffcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddc7865-9a6a-4e3d-83be-4ff5eda3c06d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7d5d61-8968-43f4-a809-09c4832de11b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, EarlyStoppingCallback, TrainerCallback, DistilBertConfig\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import get_scheduler, set_seed\n",
    "from torch.optim import AdamW\n",
    "from datasets import Dataset, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8167d1-fcfb-4cbc-a518-0a517856ff39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59765ec1-c587-4f22-a28b-423eec01ea4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5ab98-1f3f-423e-bd14-98db1e048043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f6d90-eb39-483b-95e2-ee1a4c4b2f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4b2fc8-3b70-449d-b495-7a5acc0f9b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the random seed for reproducibility\n",
    "set_seed(44)\n",
    "\n",
    "dataset = load_dataset(\"ethos\", \"binary\")\n",
    "\n",
    "# Use the dataset's train_test_split method\n",
    "train_test_dataset = dataset[\"train\"].train_test_split(test_size=0.1)\n",
    "train_dataset = train_test_dataset['train']\n",
    "val_dataset = train_test_dataset['test']\n",
    "\n",
    "# Tokenize function without additional augmentation\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts and include the labels\n",
    "    tokenized_inputs = tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
    "    tokenized_inputs['labels'] = examples['label']\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize the datasets and include the labels\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Initialize DistilBERT tokenizer and model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84429435-a6f6-42a6-b099-5ec847a59bde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results_ethosR1',\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='f1',  # Focus on F1 score\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary' if num_labels == 2 else 'macro')\n",
    "    return {'accuracy': accuracy, 'f1': f1, 'precision': precision, 'recall': recall}\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Calculate the total number of training steps\n",
    "num_training_steps = len(train_dataset) // training_args.per_device_train_batch_size * training_args.num_train_epochs\n",
    "\n",
    "# Define warmup steps as a ratio of training steps (e.g., 10% of training steps)\n",
    "warmup_steps = int(num_training_steps * 0.1)\n",
    "\n",
    "# Create the scheduler\n",
    "scheduler = get_scheduler(\n",
    "    name=\"cosine_with_restarts\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=num_training_steps,\n",
    ")\n",
    "\n",
    "\n",
    "# Define the callbacks list, assuming you have it defined\n",
    "callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "# Initialize Trainer with the custom optimizer and scheduler\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=callbacks,\n",
    "    optimizers=(optimizer, scheduler),\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0235fe-69ed-4d93-9a88-fb8d9f507907",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the model and tokenizer\n",
    "model_path = \"./model_save_runEthosR1\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "\n",
    "# Save the tokenized validation dataset\n",
    "tokenized_val_dataset.save_to_disk(\"./tokenized_validation_dataset_EthosR1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1773cd-2e2f-47a8-83b4-b0f6e515776f",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1598ab02-63f3-4da9-959b-5990a46a112d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b087d7-e658-441f-86cb-cc5de71ae7cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load feather files \n",
    "file_path = 'df_2020C_06_27_to_28_testdata.feather'\n",
    "\n",
    "# Read Feather file into a DataFrame\n",
    "df_2020C_06_27_to_28_testdata = pd.read_feather(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0ec9ae-8e33-499c-b048-18b3bbba7b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df_2020C_06_27_to_28_testdata['predicted_label'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27da46ae-dc99-4058-bdcb-60e704f5aa6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "part_size = len(df_2020C_06_27_to_28_testdata) // 40\n",
    "\n",
    "# Create a list to hold all the split DataFrames\n",
    "dfs = []\n",
    "\n",
    "# Split the DataFrame into 20 parts\n",
    "for i in range(40):\n",
    "    start_index = i * part_size\n",
    "    # Ensure the last DataFrame includes the remainder of the rows\n",
    "    if i == 39:\n",
    "        end_index = None  # This goes till the end\n",
    "    else:\n",
    "        end_index = start_index + part_size\n",
    "    split_df = df_2020C_06_27_to_28_testdata.iloc[start_index:end_index]\n",
    "    dfs.append(split_df)\n",
    "\n",
    "# You can access each DataFrame using dfs[0], dfs[1], ..., dfs[19]\n",
    "# For example, to print the first split DataFrame:\n",
    "#print(dfs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e60c7ea-0101-480d-941b-9f729ab85eca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_627_g = dfs[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a7dae4-5911-4a29-b87f-34581ff5e9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load fine-tuned tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./model_save_runEthosR1\") # THIS IS YOUR MODEL NAME SAVED FROM TESTING AND WHATEVER PATH IT IS ON\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"./model_save_runEthosR1\") # THIS IS YOUR MODEL NAME SAVED FROM TESTING\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['body'], padding='max_length', truncation=True, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ee9ba6-48ca-4971-8a76-4ccaa2e6a1b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Tokenize dataset\n",
    "texts = df_627_g['body'].astype(str).tolist()\n",
    "tokenized_inputs = tokenize_function({\"body\": texts})\n",
    "\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        #self.encodings = encodings\n",
    "        self.encodings = {key: torch.tensor(val).to(device).detach() for key, val in encodings.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            # Handle single integer index (expected behavior)\n",
    "            #return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        elif isinstance(idx, list):\n",
    "            # Handle list of indices (if passed by DataLoader)\n",
    "            #return {key: torch.stack([torch.tensor(val[i]) for i in idx]) for key, val in self.encodings.items()}\n",
    "            return {key: torch.stack([val[i].clone().detach() for i in idx]) for key, val in self.encodings.items()}\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported index type: {type(idx)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "pred_dataset = PredictionDataset(tokenized_inputs)\n",
    "\n",
    "# Create dataloader without a specified collate function (use default behavior)\n",
    "pred_dataloader = DataLoader(pred_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for batch in pred_dataloader:\n",
    "    #batch = {k: v.to('cpu') for k, v in batch.items()} # I think this can be changed to GPU should you be able to run it on a GPU\n",
    "    batch = {k: v.clone().detach().to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    preds = outputs.logits.argmax(dim=-1)\n",
    "    predictions.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbe4b05-0eed-4eff-9a81-2e0f64c23115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_627_g['predicted_label'] = predictions\n",
    "df_2020C_06_27_to_28_testdata.loc[df_627_g.index, 'predicted_label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb41c7-ef81-4579-b96d-520678d7517d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_627_g.to_csv('df_627_g--Version_2.csv', index=False)\n",
    "df_2020C_06_27_to_28_testdata.loc[df_627_g.index, 'predicted_label'].to_csv('df_627_g--Version_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7a1445-d2a6-4956-950d-ea428e72f86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_627_h = dfs[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dbb70f-eadd-47d9-9c2d-7914fcb5d0c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# Tokenize dataset\n",
    "texts = df_627_h['body'].astype(str).tolist()\n",
    "tokenized_inputs = tokenize_function({\"body\": texts})\n",
    "\n",
    "'''\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            # Handle single integer index (expected behavior)\n",
    "            return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        elif isinstance(idx, list):\n",
    "            # Handle list of indices (if passed by DataLoader)\n",
    "            return {key: torch.stack([torch.tensor(val[i]) for i in idx]) for key, val in self.encodings.items()}\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported index type: {type(idx)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "'''\n",
    "\n",
    "# Create dataset and dataloader\n",
    "pred_dataset = PredictionDataset(tokenized_inputs)\n",
    "\n",
    "# Create dataloader without a specified collate function (use default behavior)\n",
    "pred_dataloader = DataLoader(pred_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for batch in pred_dataloader:\n",
    "    #batch = {k: v.to('cpu') for k, v in batch.items()} # I think this can be changed to GPU should you be able to run it on a GPU\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    preds = outputs.logits.argmax(dim=-1)\n",
    "    predictions.extend(preds.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65ef3b0-dbda-4a9c-8a91-1fd02a275f9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#df_627_h['predicted_label'] = predictions\n",
    "df_2020C_06_27_to_28_testdata.loc[df_627_h.index, 'predicted_label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cded00-51cb-46e4-b6f7-4463b9cc6f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_2020C_06_27_to_28_testdata.loc[df_627_h.index, 'predicted_label'].to_csv('df_627_h--Version_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfada92d-29e2-486e-aca1-5da7c6f1083c",
   "metadata": {},
   "source": [
    "# Faster process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a99f335a-f620-40b7-861e-1df638e4068e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a1a7714bf4c42bcaed948dd756023b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing:   0%|          | 0/63134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ca47bd434e04075b9696d00d79d4697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/197293 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 9min 59s, sys: 1min, total: 1h 10min 59s\n",
      "Wall time: 59min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import DistilBertForSequenceClassification\n",
    "\n",
    "#Load feather files\n",
    "file_path = 'df_2020C_07_01_to_02_testdata.feather'\n",
    "\n",
    "# Read Feather file into a DataFrame\n",
    "data_frame = pd.read_feather(file_path)\n",
    "\n",
    "model_path = \"./model_save_runEthosR1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\"Using device: {device}\")\n",
    "\n",
    "# Load fine-tuned tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path, num_labels = 2)\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding = 'max_length', truncation = True, max_length = 128)\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100  # Adjust chunk size based on your system's capacity\n",
    "\n",
    "# Tokenize dataset in chunks\n",
    "texts = data_frame['body'].astype(str).tolist()\n",
    "tokenized_inputs = {'input_ids': [], 'attention_mask': []}  # Initialize your encoding structure\n",
    "\n",
    "for i in tqdm(range(0, len(texts), chunk_size), desc=\"Tokenizing\"):\n",
    "    chunk_texts = texts[i:i + chunk_size]  # Get a chunk of texts\n",
    "    chunk_tokenized = tokenize_function(chunk_texts)  # Tokenize the chunk\n",
    "    tokenized_inputs['input_ids'].extend(chunk_tokenized['input_ids'])\n",
    "    tokenized_inputs['attention_mask'].extend(chunk_tokenized['attention_mask'])\n",
    "    \n",
    "# Class for handling the tokenized data\n",
    "class PredictionDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        #self.encodings = encodings\n",
    "        self.encodings = {key: torch.tensor(val).to(device).detach() for key, val in encodings.items()}\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if isinstance(idx, int):\n",
    "            #return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "            return {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        elif isinstance(idx, list):\n",
    "            #return {key: torch.stack([torch.tensor(val[i]) for i in idx]) for key, val in self.encodings.items()}\n",
    "            return {key: torch.stack([val[i].clone().detach() for i in idx]) for key, val in self.encodings.items()}\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported index type: {type(idx)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Create dataset and dataloader\n",
    "pred_dataset = PredictionDataset(tokenized_inputs)\n",
    "pred_dataloader = DataLoader(pred_dataset, batch_size = 32, shuffle = False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = []\n",
    "for batch in tqdm(pred_dataloader, desc=\"Predicting\"):\n",
    "    #batch = {k: v.to('cpu') for k, v in batch.items()}\n",
    "    batch = {k: v.clone().detach().to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    preds = outputs.logits.argmax(dim=-1)\n",
    "    predictions.extend(preds.tolist())\n",
    "    \n",
    "# Add predictions to DataFrame and save\n",
    "data_frame['predicted_label'] = predictions\n",
    "data_frame.to_csv('Data_Frame_Of_Texts_And_Predictions_By_DBHSBC_For_2020-07-01_To_2020-07-02.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1312e0f7-1652-460e-97c3-fece3c8bfb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21a6b25-eb9f-4c3d-a0ff-f492c551cf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8473f7ca-48f0-44bb-8e28-2bda388c8dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_frame.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
